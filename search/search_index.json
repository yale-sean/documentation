{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SEAN 3","text":""},{"location":"#getting-started","title":"Getting started","text":"<p>Learn how to set up and use SEAN 3.</p>"},{"location":"#sensor-demo","title":"Sensor Demo","text":"<p>Run a simple interaction scenario using SEAN 3 with its built-in sensors.</p>"},{"location":"#reinforcement-learning-demo","title":"Reinforcement Learning Demo","text":"<p>Run a reinforcement learning scenario using SEAN 3 with multiple agents.</p>"},{"location":"getting_started/","title":"Getting Started with SEAN 3","text":"<p>SEAN 3's architecture consists of two main components: Unity and ROS noetic. In this guide, we will install both components on your system.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":""},{"location":"getting_started/#system-requirements","title":"System Requirements","text":"<p>SEAN 3 is tested on Windows 10/11 with WSL2 enabled. Ensure that you have WSL2 set up on your machine. You can follow the official Microsoft guide to install WSL2: https://docs.microsoft.com/en-us/windows/wsl/install. Install the Ubuntu 20.04 distribution in WSL for compatibility with ROS noetic.</p>"},{"location":"getting_started/#unity-setup","title":"Unity Setup","text":"<p>Install Unity Hub from https://unity.com/download</p> <p>Install Unity version 6000.0.47f1</p> <p>clone the following repo into some Unity Project folder</p> <pre><code>git@github.com:yale-img/social_sim_unity.git\n</code></pre> <p>make sure you are on hdrp-rl branch</p> <p>Now, since the UnitySensors Package is a submodule in <code>~\\social_sim_unity\\Assets\\ExternalAssets\\UnitySensors</code> run</p> <pre><code>git submodule update --init --recursive\n</code></pre> <p>if you navigate to the UnitySensors repo, you should be tracking the  <code>dev_v2.0.4</code> branch</p>"},{"location":"getting_started/#wsl-side-with-ros-noetic","title":"WSL side with ROS noetic","text":"<p>Install preconfigured workspace</p> <pre><code>git clone https://github.com/yale-img/sim_ws ~/sim_ws\n</code></pre> <p>Then, run the installer scripts to install all required packages:</p> <pre><code>cd ~/sim_ws/\n./scripts/setup.sh\n</code></pre> <p>In your base environment, install custom tkinter for the GUI interface</p> <pre><code>pip install customtkinter\n</code></pre> <ul> <li>Ensure that both your <code>~/sim_ws</code> and <code>roscd social_sim_ros</code> git branches are on <code>hdrp-rl</code></li> </ul> <p>Open-AI Gym Setup</p> <p>Install <code>uv</code>: https://docs.astral.sh/uv/getting-started/installation/#standalone-installer</p> <p>Clone the following repo into <code>~/sim_ws/src/</code></p> <p>https://github.com/yale-img/social_sim_multiagent</p> <p>and run </p> <pre><code>uv sync\nuv pip install -e multi-agent-envs/\n</code></pre> <p>Navigate to <code>/sim_ws/tmux/multi_agent_sean</code> folder to start tmuxinator</p> <pre><code>cd ~/sim_ws/tmux/multi_agent_sean\ntmuxinator\n</code></pre>"},{"location":"getting_started/#the-tmuxinatoryml-launches-the-following-files","title":"The <code>.tmuxinator.yml</code> launches the following files","text":"<ul> <li><code>rosrun social_sim_ros map_publisher.py</code> <ul> <li>Note: this launches the map_publisher, it is MANUALLY SET to warehouse for testing purposes so be sure to only use Unity warehouse scene</li> </ul> </li> <li><code>rosrun social_sim_ros multi_agent_tcp_server.py</code><ul> <li>Note: this starts the ROS tcp server for registering all publishers, transforms, services, and publishers with Unity. When this is launched, it expects a <code>ros_unity_config.json</code> file in the <code>social_sim_ros\\social_sim_ros\\src</code> folder which is automatically created as soon as you hit save config in the sean interface gui. This runs on port 10000.</li> </ul> </li> <li><code>rosrun rviz rviz -d $(rospack find social_sim_ros)/config/multi_agent.rviz</code><ul> <li>Note: this launches a custom rviz config, currently hardcoded to two agents to visualize pointclouds, TF, and laserscans</li> </ul> </li> <li><code>python3 $(rospack find social_sim_ros)/src/sean_interface.py</code> <ul> <li>Note: this launches the sean_interface gui where you should specify how many robots you want, what types, and what sensors. Also, you can enable synchronous mode and tick duration here. </li> </ul> </li> <li><code>conda activate marl &amp;&amp; cd $HOME/sim_ws/src/social_sim_multiagent &amp;&amp; sleep 5 &amp;&amp; python3 maddpg-pytorch/main.py a</code><ul> <li>This is the openaigym-ros interface which instantiates a synchronous tcp server whenever sync mode is true. client.tick() must be called before env.reset() and env.step(). If there are no messages that say Odom received and Scan received in this window, it is likely that unity did not initialize all agents and topics before the first tick message was sent. Adjust sleep as necessary.</li> </ul> </li> <li><code>sleep 5 &amp;&amp; rosrun social_sim_ros plot_robot_trajectory.py</code><ul> <li>This just visualizes path markers for the <code>/robot/i/cmd_vel</code> topics in rviz so you can track the path of the RL policy. </li> </ul> </li> </ul>"},{"location":"getting_started/#some-tips","title":"Some Tips:","text":"<p>The topics for each robot are prefixed <code>/robot/{i}/</code> and the frames are prefixed <code>robot_{i}_</code> </p> <p>Synchronous mode allows you to pause and unpause the ROSClock that is publishing in Unity. Unity will perform physics fixed updates as much as possible before tick_duration elapses in unity simulation time. To control the fixed timestepping of the Unity environment you need to run client.tick() imported from synchronous_tcp_server (SychronousTcpServer class). </p> <ul> <li>Note: When the simulation is paused in synchronous mode, unity will not receive or publish any messages. The sychronous tcp server runs on port 8080 so make sure that port forwarding rules are enabled for 8080.</li> </ul> <p>If you want to reset the agent initialization settings, note that the sean_interface gui resets parameters FOR THE NEXT RUN unless there is no config json available at which the multi_agent_tcp_server.py will hang and complain that no such file is available. If this is the case, you should generate configuration, close the tmux terminal, relaunch tmuxinator, then hit play in Unity.</p> <p>The order of execution should always be:</p> <ol> <li>If <code>ros_unity_config.json</code> is present: then</li> </ol> <pre><code>cd ~/sim_ws/tmux/multi_agent_sean\ntmuxinator\n</code></pre> <ol> <li>Hit play in Unity, at which point you should see initialization messages in Unity console. </li> <li>If you want to change the configuration for the next run, save config in SEAN Configurator GUI and <code>tmux kill-session</code> and unclick Play in Unity, then return to step 1.</li> </ol>"},{"location":"getting_started/#_1","title":"Getting Started","text":""},{"location":"rl_demo/","title":"Reinforcement Learning Demo","text":"<p>This demo shows how to train a reinforcement learning policy using SEAN 3 with multiple agents.</p>"}]}